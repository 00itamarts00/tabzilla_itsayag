{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Aggregate results for analysis\n",
    "\n",
    "Read the complete metadataset and generate a clean version for analysis.\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "1. make sure that the latest metadataset is present in the `TabSurvey` folder (two levels up from this script). The latest metadataset can be downloaded from the GCP bucket here: `tabzilla-results/metadatasets`.\n",
    "\n",
    "2. Run the first two cells in thie notebook, to create a metadataset `metadataset_df`. This dataframe includes all results from our experiments on datasets with clasisfication and binary targets. The remaining cells are divided into two sections: (A) and (B).\n",
    "\n",
    "3. Modify and run the cells in section (A) to create a dataframe `analysis_df`, which is a subset of `metadataset_df`. This subset should include all results you want to include in later analyses. Section (A) in this notebook selects results such that each algorithm has results for at least 85% of all datasets.\n",
    "\n",
    "4. After `analysis_df` is defined in section (A), run cells in section (B) to \"tune\" all algorithms on each dataset, and calculate normalized results and rankings. These cells create four \"cleaned\" results files, which will be used for all subsequent analysis: \n",
    "- `./cleaned_results/tuned_aggregated_results.csv`: performance of each tuned algorithm on each dataset, where performance is averaged over all 10 folds. \n",
    "- `./cleaned_results/tuned_fold_results.csv`: performance of each tuned algorithm on each dataset fold.\n",
    "- `./cleaned_results/tuned_aggregated_results_with_default.csv`: same as `tuned_aggregated_results_with_default.csv`, but with the default hyperparameters of each dataset included as a separate algorithm\n",
    "- `./cleaned_results/tuned_fold_results_with_default.csv`: same as `tuned_fold_results_with_default.csv`, but with the default hyperparameters of each dataset included as a separate algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from analysis_utils import get_tuned_alg_perf\n",
    "\n",
    "metadata_folder = Path(\"../../TabSurvey\")\n",
    "\n",
    "metadataset_df = pd.read_csv(metadata_folder / \"metadataset.csv\")\n",
    "errors_df = pd.read_csv(metadata_folder / \"metadataset_errors.csv\")\n",
    "\n",
    "# keep only binary and classification datasets. we have some results for regression datasets, which are not used.\n",
    "metadataset_df = metadataset_df.loc[metadataset_df[\"target_type\"].isin([\"binary\", \"classification\"]), :]\n",
    "\n",
    "# make sure that the cleaned_results folder exists\n",
    "output_folder = Path(\"./cleaned_results\")\n",
    "output_folder.mkdir(exist_ok=True)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print number of results per dataset and alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for each alg: number of datasets with results (out of 176)\n",
      "alg_name\n",
      "TabPFNModel            63\n",
      "NAM                    80\n",
      "DeepFM                 90\n",
      "TabTransformer        124\n",
      "SAINT                 138\n",
      "NODE                  141\n",
      "SVM                   143\n",
      "DANet                 147\n",
      "rtdl_FTTransformer    148\n",
      "VIME                  163\n",
      "STG                   164\n",
      "CatBoost              165\n",
      "LightGBM              165\n",
      "KNN                   167\n",
      "LinearModel           168\n",
      "TabNet                168\n",
      "RandomForest          173\n",
      "XGBoost               174\n",
      "rtdl_ResNet           174\n",
      "MLP                   175\n",
      "DecisionTree          175\n",
      "rtdl_MLP              176\n",
      "Name: dataset_name, dtype: int64\n",
      "for each dataset: number of algs with results (out of 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset_name\n",
       "openml__poker-hand__9890             5\n",
       "openml__covertype__7593              7\n",
       "openml__Devnagari-Script__167121     8\n",
       "openml__albert__189356               9\n",
       "openml__CIFAR_10__167124            10\n",
       "                                    ..\n",
       "openml__profb__3561                 22\n",
       "openml__dresses-sales__125920       22\n",
       "openml__pc1__3918                   22\n",
       "openml__heart-h__50                 22\n",
       "openml__kc2__3913                   22\n",
       "Name: alg_name, Length: 176, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each alg, for how many datasets are there results?\n",
    "print(f\"for each alg: number of datasets with results (out of {len(metadataset_df['dataset_name'].unique())})\")\n",
    "print(metadataset_df.groupby(\"alg_name\")[\"dataset_name\"].apply(lambda x: len(set(x))).sort_values())\n",
    "\n",
    "print(f\"for each dataset: number of algs with results (out of {len(metadataset_df['alg_name'].unique())})\")\n",
    "metadataset_df.groupby(\"dataset_name\")[\"alg_name\"].apply(lambda x: len(set(x))).sort_values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Dataset inclusion/exclusion\n",
    "\n",
    "**In this notebook: selected-18-algs:**\n",
    "* We use a list of 18 algs (excluding 3 that had lots of errors.)\n",
    "* We take only the datasets where each of these algs produce a result. This is ~100 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for each dataset: number of algs with results\n",
      "dataset_name\n",
      "openml__poker-hand__9890             5\n",
      "openml__covertype__7593              7\n",
      "openml__Devnagari-Script__167121     8\n",
      "openml__albert__189356               9\n",
      "openml__CIFAR_10__167124            10\n",
      "Name: alg_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"for each dataset: number of algs with results\")\n",
    "alg_counts = metadataset_df.groupby(\"dataset_name\")[\"alg_name\"].agg(lambda x: len(set(x))).sort_values()\n",
    "print(alg_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeping 104 datasets\n"
     ]
    }
   ],
   "source": [
    "# drop:\n",
    "# - TabPFN (only runs for small datasets)\n",
    "# - NAM (lots of errors, long runtime)\n",
    "# - DeepFM (not implemented for multi-class)\n",
    "\n",
    "selected_algs = [\n",
    "    \"SAINT\",\n",
    "    \"NODE\",\n",
    "    \"SVM\",\n",
    "    \"DANet\",\n",
    "    \"rtdl_FTTransformer\",\n",
    "    \"VIME\",\n",
    "    \"STG\",\n",
    "    \"CatBoost\",\n",
    "    \"LightGBM\",\n",
    "    \"KNN\",\n",
    "    \"LinearModel\",\n",
    "    \"TabNet\",\n",
    "    \"RandomForest\",\n",
    "    \"XGBoost\",\n",
    "    \"rtdl_ResNet\",\n",
    "    \"MLP\",\n",
    "    \"DecisionTree\",\n",
    "    \"rtdl_MLP\",\n",
    "    # \"NAM\",\n",
    "    # \"TabPFNModel\",\n",
    "    # \"DeepFM\",\n",
    "]\n",
    "\n",
    "test_df = metadataset_df.loc[metadataset_df[\"alg_name\"].isin(selected_algs), :]\n",
    "\n",
    "# keep only datasets where all selected algs produce a result\n",
    "alg_count = test_df.groupby(\"dataset_name\")[\"alg_name\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "keep_datasets = alg_count[alg_count == len(selected_algs)].index\n",
    "\n",
    "print(f\"keeping {len(keep_datasets)} datasets\")\n",
    "\n",
    "keep_df = test_df.loc[test_df[\"dataset_name\"].isin(keep_datasets), :]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly more data prep\n",
    "\n",
    "Note: We will keep all algs, regardless of how many datasets they have results for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing datasets: number of datasets with results\n",
      "alg_name\n",
      "CatBoost              104\n",
      "rtdl_FTTransformer    104\n",
      "XGBoost               104\n",
      "VIME                  104\n",
      "TabNet                104\n",
      "SVM                   104\n",
      "STG                   104\n",
      "SAINT                 104\n",
      "RandomForest          104\n",
      "NODE                  104\n",
      "MLP                   104\n",
      "LinearModel           104\n",
      "LightGBM              104\n",
      "KNN                   104\n",
      "DecisionTree          104\n",
      "DANet                 104\n",
      "rtdl_MLP              104\n",
      "rtdl_ResNet           104\n",
      "Name: dataset_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "analysis_df = keep_df\n",
    "\n",
    "print(\"after removing datasets: number of datasets with results\")\n",
    "dataset_counts = analysis_df.groupby(\"alg_name\")[\"dataset_name\"].agg(lambda x: len(set(x))).sort_values()\n",
    "print(dataset_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Tune and rank algorithms for each dataset\n",
    "\n",
    "**Note**: At this point, you should have a dataframe called `analysis_df`, which contains all results you want to include in the remainder of the analysis. \n",
    "\n",
    "The code below performs hyperparameter tuning & ranking of each alg, and writes four cleaned results files to the directory `./cleaned_results`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = [\n",
    "    \"Accuracy\",\n",
    "    \"F1\",\n",
    "    \"Log Loss\",\n",
    "]\n",
    "\n",
    "obj_type_list = [\n",
    "    \"maximize\",\n",
    "    \"maximize\",\n",
    "    \"minimize\",\n",
    "]\n",
    "result_df_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookkeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32643/1589380835.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  analysis_df.loc[:, \"alg_name\"] = analysis_df[\"alg_name\"].apply(lambda x: ALG_DISPLAY_NAMES[x])\n",
      "/tmp/ipykernel_32643/1589380835.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  analysis_df.loc[:, \"alg_type\"] = analysis_df[\"alg_name\"].apply(lambda x: ALG_TYPES[x])\n"
     ]
    }
   ],
   "source": [
    "# replace alg name with display name\n",
    "from analysis_utils import ALG_DISPLAY_NAMES, ALG_TYPES\n",
    "analysis_df.loc[:, \"alg_name\"] = analysis_df[\"alg_name\"].apply(lambda x: ALG_DISPLAY_NAMES[x])\n",
    "\n",
    "# add alg type\n",
    "analysis_df.loc[:, \"alg_type\"] = analysis_df[\"alg_name\"].apply(lambda x: ALG_TYPES[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a copy of each \"default\" hparam row, to treat this as a separate alg\n",
    "default_rows = analysis_df.loc[analysis_df[\"hparam_source\"] == \"default\"].copy()\n",
    "default_rows.loc[:, \"alg_name\"] = default_rows[\"alg_name\"].apply(lambda x: x + \" (default)\")\n",
    "\n",
    "# remove TabPFN and LinearModel, since these only have one hparam set\n",
    "default_rows = default_rows.loc[~(default_rows[\"alg_name\"].str.contains(\"TabPFNModel\") | default_rows[\"alg_name\"].str.contains(\"LinearModel\")), :]\n",
    "\n",
    "# append these to the metadataset\n",
    "analysis_df_with_default = pd.concat([analysis_df, default_rows], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### produce cleaned results files\n",
    "\n",
    "first, each algorithm is tuned for each dataset fold (10 folds per dataset), using all available hyperparameter samples. we then calculate the normalized and ranked performance for each algorithm over all datasets.\n",
    "\n",
    "the following loop produces four files:\n",
    "- `./cleaned_results/tuned_aggregated_results.csv`: performance of each tuned algorithm on each dataset, where performance is averaged over all 10 folds. \n",
    "- `./cleaned_results/tuned_fold_results.csv`: performance of each tuned algorithm on each dataset fold.\n",
    "- `./cleaned_results/tuned_aggregated_results_with_default.csv`: same as `tuned_aggregated_results_with_default.csv`, but with the default hyperparameters of each dataset included as a separate algorithm\n",
    "- `./cleaned_results/tuned_fold_results_with_default.csv`: same as `tuned_fold_results_with_default.csv`, but with the default hyperparameters of each dataset included as a separate algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_result_dfs = {}\n",
    "for drop_default in [True, False]:\n",
    "    for i, (metric, objective_type) in enumerate(zip(metric_list, obj_type_list)):\n",
    "\n",
    "        test_metric_col = metric + \"__test\"\n",
    "\n",
    "        if drop_default:\n",
    "            df = analysis_df.copy()\n",
    "        else:\n",
    "            df = analysis_df_with_default.copy()\n",
    "\n",
    "        tuned_alg_perf = get_tuned_alg_perf(df, metric=metric)\n",
    "\n",
    "        # NOTE: this \"tunes\" each algorithm for each training fold separately. so each of the 10 folds might use different hparams.\n",
    "        tuned_result_dfs[metric] = tuned_alg_perf\n",
    "\n",
    "        ##############################\n",
    "        ### STEP 1: TREAT EACH FOLD AS SEPARATE DATASET\n",
    "\n",
    "        result_col = test_metric_col\n",
    "        \n",
    "        # for each dataset, find the min and max metrics over all tuned algs\n",
    "        overall_bounds = tuned_alg_perf.groupby(\"dataset_fold_id\").agg({result_col: [\"min\", \"max\"]}).reset_index()\n",
    "\n",
    "        # rename the multiindex cols\n",
    "        new_cols = []\n",
    "        for c in overall_bounds.columns:\n",
    "            if c[1] == \"\":\n",
    "                new_cols.append(c[0])\n",
    "            else:\n",
    "                new_cols.append(\"_\".join(c))\n",
    "\n",
    "        overall_bounds.columns = new_cols\n",
    "\n",
    "        tuned_alg_perf = tuned_alg_perf.merge(overall_bounds, on=\"dataset_fold_id\", how=\"left\")\n",
    "\n",
    "        # add normalized metric\n",
    "        tuned_alg_perf.loc[:, \"normalized_\" + result_col] = (tuned_alg_perf[result_col] - tuned_alg_perf[result_col + \"_min\"]) / (tuned_alg_perf[result_col + \"_max\"] - tuned_alg_perf[result_col + \"_min\"])\n",
    "\n",
    "        # rank all algs for each dataset\n",
    "        ascending = False if objective_type == \"maximize\" else True\n",
    "        \n",
    "        tuned_alg_perf.loc[:, f\"{metric}_rank\"] = tuned_alg_perf.groupby([\"dataset_fold_id\"])[result_col].rank(method=\"min\", ascending=ascending).values\n",
    "\n",
    "        # keep these cols to merge\n",
    "        merge_cols = [\n",
    "            \"alg_name\", \n",
    "            \"dataset_fold_id\", \n",
    "            \"normalized_\" + result_col,\n",
    "            f\"{metric}_rank\",\n",
    "            result_col + \"_min\",\n",
    "            result_col + \"_max\"\n",
    "        ]\n",
    "\n",
    "        if i == 0:\n",
    "            fold_tuned_df = tuned_alg_perf.copy()\n",
    "        else:\n",
    "            fold_tuned_df = fold_tuned_df.merge(tuned_alg_perf[merge_cols], on=[\"alg_name\", \"dataset_fold_id\"])\n",
    "\n",
    "        ##############################\n",
    "        ### STEP 2: AVERAGE OVER FOLDS\n",
    "\n",
    "        if i == 0:\n",
    "            agg_dict = {\n",
    "                test_metric_col: [\"median\", \"mean\"],\n",
    "                \"time__train\": [\"median\", \"mean\"],\n",
    "                # \"dataset_name\": [\"count\"],\n",
    "            }\n",
    "        else:\n",
    "            agg_dict = {\n",
    "                test_metric_col: [\"median\", \"mean\"],\n",
    "            }\n",
    "\n",
    "        # aggregate over folds: take the mean & median performance over each fold\n",
    "        agg_tuned_alg_perf = tuned_alg_perf.groupby([\"alg_name\", \"dataset_name\"]).agg(agg_dict).reset_index()\n",
    "\n",
    "        # rename the multiindex cols\n",
    "        new_cols = []\n",
    "        for c in agg_tuned_alg_perf.columns:\n",
    "            if c[1] == \"\":\n",
    "                new_cols.append(c[0])\n",
    "            else:\n",
    "                new_cols.append(\"_\".join(c))\n",
    "\n",
    "        agg_tuned_alg_perf.columns = new_cols\n",
    "\n",
    "\n",
    "        # define the target metric column, we will use this value for all plots\n",
    "        result_col = test_metric_col + \"_mean\"\n",
    "\n",
    "        # for each dataset, find the min and max metrics over all tuned algs\n",
    "        overall_bounds = agg_tuned_alg_perf.groupby(\"dataset_name\").agg({result_col: [\"min\", \"max\"]}).reset_index()\n",
    "        \n",
    "        # rename the multiindex cols\n",
    "        new_cols = []\n",
    "        for c in overall_bounds.columns:\n",
    "            if c[1] == \"\":\n",
    "                new_cols.append(c[0])\n",
    "            else:\n",
    "                new_cols.append(\"_\".join(c))\n",
    "\n",
    "        overall_bounds.columns = new_cols\n",
    "\n",
    "        \n",
    "        agg_tuned_alg_perf = agg_tuned_alg_perf.merge(overall_bounds, on=\"dataset_name\", how=\"left\")\n",
    "\n",
    "        # add normalized metric\n",
    "        agg_tuned_alg_perf.loc[:, \"normalized_\" + result_col] = (agg_tuned_alg_perf[result_col] - agg_tuned_alg_perf[result_col + \"_min\"]) / (agg_tuned_alg_perf[result_col + \"_max\"] - agg_tuned_alg_perf[result_col + \"_min\"])\n",
    "\n",
    "        # rank all algs for each dataset\n",
    "        ascending = False if objective_type == \"maximize\" else True\n",
    "        \n",
    "        # rank according to mean performance over all folds\n",
    "        agg_method = \"mean\"\n",
    "\n",
    "        # rank everything\n",
    "        agg_tuned_alg_perf.loc[:, f\"{metric}_rank_{agg_method}\"]  = \\\n",
    "            agg_tuned_alg_perf.groupby([\"dataset_name\"])[test_metric_col + \"_\" + agg_method].rank(method=\"min\", ascending=ascending).values\n",
    "\n",
    "\n",
    "        # keep these cols to merge\n",
    "        merge_cols = [\n",
    "            \"alg_name\", \n",
    "            \"dataset_name\",\n",
    "            \"normalized_\" + result_col,\n",
    "            f\"{metric}_rank_mean\",\n",
    "            result_col + \"_min\",\n",
    "            result_col + \"_max\"\n",
    "        ]\n",
    "\n",
    "        if i == 0:\n",
    "            tuned_agg_df = agg_tuned_alg_perf.copy()\n",
    "        else:\n",
    "            tuned_agg_df = tuned_agg_df.merge(agg_tuned_alg_perf[merge_cols], on=[\"alg_name\", \"dataset_name\"])\n",
    "\n",
    "    # save results\n",
    "\n",
    "    # merge in alg type, for bookkeeping\n",
    "    alg_type_df = analysis_df[[\"alg_name\", \"alg_type\"]].drop_duplicates()\n",
    "    tuned_agg_df = tuned_agg_df.merge(alg_type_df, on=\"alg_name\", how=\"left\")\n",
    "    fold_tuned_df = fold_tuned_df.merge(alg_type_df, on=\"alg_name\", how=\"left\")\n",
    "\n",
    "    if drop_default:\n",
    "        agg_df_no_default = tuned_agg_df.copy()\n",
    "        agg_df_no_default.to_csv(\"./cleaned_results/tuned_aggregated_results.csv\")\n",
    "\n",
    "        tuned_fold_df_no_default = fold_tuned_df.copy()\n",
    "        tuned_fold_df_no_default.to_csv(\"./cleaned_results/tuned_fold_results.csv\")\n",
    "       \n",
    "    else:\n",
    "        agg_df_with_default = tuned_agg_df.copy()\n",
    "        agg_df_with_default.to_csv(\"./cleaned_results/tuned_aggregated_results_with_default.csv\")\n",
    "\n",
    "        tuned_fold_df_with_default = fold_tuned_df.copy()\n",
    "        tuned_fold_df_with_default.to_csv(\"./cleaned_results/tuned_fold_results_with_default.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>Accuracy__test_median</th>\n",
       "      <th>Accuracy__test_mean</th>\n",
       "      <th>time__train_median</th>\n",
       "      <th>time__train_mean</th>\n",
       "      <th>Accuracy__test_mean_min</th>\n",
       "      <th>Accuracy__test_mean_max</th>\n",
       "      <th>normalized_Accuracy__test_mean</th>\n",
       "      <th>Accuracy_rank_mean</th>\n",
       "      <th>normalized_F1__test_mean</th>\n",
       "      <th>F1_rank_mean</th>\n",
       "      <th>F1__test_mean_min</th>\n",
       "      <th>F1__test_mean_max</th>\n",
       "      <th>normalized_Log Loss__test_mean</th>\n",
       "      <th>Log Loss_rank_mean</th>\n",
       "      <th>Log Loss__test_mean_min</th>\n",
       "      <th>Log Loss__test_mean_max</th>\n",
       "      <th>alg_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Australian__146818</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>1.347650</td>\n",
       "      <td>1.393643</td>\n",
       "      <td>0.711594</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.711594</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.302677</td>\n",
       "      <td>0.755920</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Click_prediction_small__190408</td>\n",
       "      <td>0.839655</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>14.031593</td>\n",
       "      <td>16.126213</td>\n",
       "      <td>0.831581</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.831581</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.409856</td>\n",
       "      <td>5.816984</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__LED-display-domain-7digit__125921</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.696405</td>\n",
       "      <td>1.113755</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.693412</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.697046</td>\n",
       "      <td>0.731609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827409</td>\n",
       "      <td>2.539521</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__MiceProtein__146800</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.980556</td>\n",
       "      <td>2.073627</td>\n",
       "      <td>3.307141</td>\n",
       "      <td>0.661111</td>\n",
       "      <td>0.998148</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.954091</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.614618</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.076744</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>1.380437</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__PhishingWebsites__14952</td>\n",
       "      <td>0.961104</td>\n",
       "      <td>0.960561</td>\n",
       "      <td>5.313641</td>\n",
       "      <td>5.994810</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.972953</td>\n",
       "      <td>0.728706</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.728706</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.972953</td>\n",
       "      <td>0.148074</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.070005</td>\n",
       "      <td>0.192443</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alg_name                               dataset_name  Accuracy__test_median  \\\n",
       "0  CatBoost                 openml__Australian__146818               0.869565   \n",
       "1  CatBoost     openml__Click_prediction_small__190408               0.839655   \n",
       "2  CatBoost  openml__LED-display-domain-7digit__125921               0.720000   \n",
       "3  CatBoost                openml__MiceProtein__146800               0.981481   \n",
       "4  CatBoost            openml__PhishingWebsites__14952               0.961104   \n",
       "\n",
       "   Accuracy__test_mean  time__train_median  time__train_mean  \\\n",
       "0             0.872464            1.347650          1.393643   \n",
       "1             0.838565           14.031593         16.126213   \n",
       "2             0.728000            0.696405          1.113755   \n",
       "3             0.980556            2.073627          3.307141   \n",
       "4             0.960561            5.313641          5.994810   \n",
       "\n",
       "   Accuracy__test_mean_min  Accuracy__test_mean_max  \\\n",
       "0                 0.711594                 0.872464   \n",
       "1                 0.831581                 0.838565   \n",
       "2                 0.698000                 0.736000   \n",
       "3                 0.661111                 0.998148   \n",
       "4                 0.927273                 0.972953   \n",
       "\n",
       "   normalized_Accuracy__test_mean  Accuracy_rank_mean  \\\n",
       "0                        1.000000                 1.0   \n",
       "1                        1.000000                 1.0   \n",
       "2                        0.789474                 4.0   \n",
       "3                        0.947802                 7.0   \n",
       "4                        0.728706                12.0   \n",
       "\n",
       "   normalized_F1__test_mean  F1_rank_mean  F1__test_mean_min  \\\n",
       "0                  1.000000           1.0           0.711594   \n",
       "1                  1.000000           1.0           0.831581   \n",
       "2                  0.693412           5.0           0.697046   \n",
       "3                  0.954091           7.0           0.614618   \n",
       "4                  0.728706          12.0           0.927273   \n",
       "\n",
       "   F1__test_mean_max  normalized_Log Loss__test_mean  Log Loss_rank_mean  \\\n",
       "0           0.872464                        0.000000                 1.0   \n",
       "1           0.838565                        0.000000                 1.0   \n",
       "2           0.731609                        0.000000                 1.0   \n",
       "3           0.998147                        0.076744                 9.0   \n",
       "4           0.972953                        0.148074                 8.0   \n",
       "\n",
       "   Log Loss__test_mean_min  Log Loss__test_mean_max alg_type  \n",
       "0                 0.302677                 0.755920     gbdt  \n",
       "1                 0.409856                 5.816984     gbdt  \n",
       "2                 0.827409                 2.539521     gbdt  \n",
       "3                 0.011987                 1.380437     gbdt  \n",
       "4                 0.070005                 0.192443     gbdt  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek\n",
    "agg_df_no_default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "367a26e3fad5ec8f1d0bebe9545860115b1157dbd92bed67faee9b31d4dbfaa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
